@proceedings{2023.eamt-1.0,
    author = "Mary Nurminen and Judith Brenner and Maarit Koponen and Sirkku Latomaa and Mikhail Mikhailov and Frederike Schierl and Tharindu Ranasinghe and Eva Vanmassenhove and Sergi Alvarez Vidal and Nora Aranberri and Mara Nunziatini and Carla Parra Escart\'{\i}n and Mikel Forcada and Maja Popovic and Carolina Scarton and Helena Moniz",
    title = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    year = "2023",
    month = "June",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation"
}

@inproceedings{2023.eamt-1.1,
    author = "Biao Zhang",
    title = "Towards Efficient Universal Neural Machine Translation",
    year = "2023",
    month = "June",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    pages = "4--5",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation"
}

@inproceedings{2023.eamt-1.2,
    author = "Javad Pourmostafa Roshan Sharami and Dimitar Shterionov and Fr\'{e}d\'{e}ric Blain and Eva Vanmassenhove and Mirella De Sisto and Chris Emmery and Pieter Spronck",
    title = "Tailoring Domain Adaptation for Machine Translation Quality Estimation",
    year = "2023",
    month = "June",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    pages = "7--18",
    abstract = "While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizabile, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues --- data scarcity and domain mismatch --- this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation"
}


